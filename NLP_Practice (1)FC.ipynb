{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical 1"
      ],
      "metadata": {
        "id": "IhOhh6iLUgE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write python script to convert given input text to speech."
      ],
      "metadata": {
        "id": "51eslTWUUk_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgnZxhdTSteG"
      },
      "outputs": [],
      "source": [
        "# Check if the required module is installed.\n",
        "!pip show gtts\n",
        "\n",
        "# If the module is not installed, install it.\n",
        "!pip install gtts\n",
        "\n",
        "# Import the required modules.\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "# Define the function to convert text to speech.\n",
        "def text_to_speech(text, language='en', slow=False):\n",
        "  speech = gTTS(text=text, lang=language, slow=slow)\n",
        "  speech.save(\"output.mp3\")\n",
        "  os.system(\"mpg321 output.mp3\")\n",
        "\n",
        "# Define the text to be converted to speech.\n",
        "text = \"Hello, this is a sample text-to-speech conversion.\"\n",
        "\n",
        "# Call the function to convert the text to speech.\n",
        "text_to_speech(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Write python script to convert given input speech to text."
      ],
      "metadata": {
        "id": "QuPiS9gYYjVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speech_recognition"
      ],
      "metadata": {
        "id": "aZjvjE1LadOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "filename=\"C:/Users/tcsc/Downloads/Alice_Arnold_voice.ogg\"\n",
        "r=sr.Recognizer ()\n",
        "with sr.AudioFile (filename) as source:\n",
        " audio_data=r.record (source)\n",
        " text=r.recognize_google (audio_data)\n",
        " print (text)"
      ],
      "metadata": {
        "id": "SufFjR4tUzvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Work"
      ],
      "metadata": {
        "id": "Fq6eJc-6s6oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGrfqutEsUw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL NO. 2"
      ],
      "metadata": {
        "id": "qc9jbv1raovz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Study of various corpus-Brown, Inaugural, Reuters, UDHR with various methods like fields,\n",
        "raw, words, sents, categories"
      ],
      "metadata": {
        "id": "uUuwjCgItL3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "mYN1YfO6aP-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words()"
      ],
      "metadata": {
        "id": "1s7NPpjwaPhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.categories()"
      ],
      "metadata": {
        "id": "BUM3rOFQaPRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(categories='romance')"
      ],
      "metadata": {
        "id": "HD-oy2rnuoDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(categories='editorial')"
      ],
      "metadata": {
        "id": "Q6EV-76QuyAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.fileids()"
      ],
      "metadata": {
        "id": "7ZAC_FPyvaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(fileids=['cr07'])\n"
      ],
      "metadata": {
        "id": "GDl-rsbqvh7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.sents()"
      ],
      "metadata": {
        "id": "o2VZbRHbwBIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Study Conditional Frequency Distribution"
      ],
      "metadata": {
        "id": "0J7iWLPSwz_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text = brown.words(categories='news')"
      ],
      "metadata": {
        "id": "ygAKFJUdwK3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist=nltk.FreqDist([w.lower() for w in news_text])"
      ],
      "metadata": {
        "id": "iFAM8482x6Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modals=['can', 'could', 'may', 'might', 'must', 'will']\n",
        "for m in modals:\n",
        "  print(m,\"\",fdist[m])"
      ],
      "metadata": {
        "id": "bw-CHFafyJsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConditionalFreqDist"
      ],
      "metadata": {
        "id": "nDg0DQH7zW9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Create a ConditionalFreqDist\n",
        "cfd = nltk.ConditionalFreqDist((genre, word)\n",
        "                               for genre in brown.categories()\n",
        "                               for word in brown.words(categories=genre))\n",
        "\n",
        "# Define genres and modals\n",
        "genres = ['news', 'religion', 'fiction', 'thriller', 'romance']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "\n",
        "# Tabulate the data\n",
        "cfd.tabulate(conditions=genres, samples=modals)\n"
      ],
      "metadata": {
        "id": "hrBH6tCRybYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL NO. 3"
      ],
      "metadata": {
        "id": "f5aa29ql5GkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Create and use your own corpora (plain text).\n"
      ],
      "metadata": {
        "id": "dZQBUQfG_kSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "# Specify the corpus root directory\n",
        "corpus_root = '/content/Msc.txt'\n",
        "\n",
        "# Instantiate PlaintextCorpusReader\n",
        "filelist = PlaintextCorpusReader(corpus_root, '.*')\n",
        "\n",
        "\n",
        "print('\\nFile list:\\n', filelist.fileids())\n",
        "print('\\nFilelist Root:', filelist.root)\n",
        "\n",
        "\n",
        "w = filelist.words('Msc.txt')\n",
        "print('\\nFirst 6 words:', w[:6])\n",
        "\n",
        "\n",
        "w1 = filelist.sents('Msc.txt')\n",
        "print('\\nFirst sentence:', w1[0])\n",
        "\n",
        "\n",
        "\n",
        "for fileid in filelist.fileids():\n",
        "    num_chars = len(filelist.raw(fileid))\n",
        "    num_words = len(filelist.words(fileid))\n",
        "    num_sents = len(filelist.sents(fileid))\n",
        "    num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))\n",
        "\n",
        "    # Print statistics\n",
        "    print(int(num_chars / num_words), '\\t\\t\\t', int(num_words / num_sents), '\\t\\t\\t',\n",
        "          int(num_words / num_vocab), '\\t\\t\\t', fileid)\n"
      ],
      "metadata": {
        "id": "jNdysqArzT9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Study of tagged corpora with methods like tagged_sents, tagged_words."
      ],
      "metadata": {
        "id": "DEgAuqjt_hhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required datasets\n",
        "nltk.download('conll2000')\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Load the tagged words from the Brown, CoNLL 2000, and Treebank corpora\n",
        "brown_tagged_words = nltk.corpus.brown.tagged_words()\n",
        "conll2000_tagged_words = nltk.corpus.conll2000.tagged_words()\n",
        "treebank_tagged_words = nltk.corpus.treebank.tagged_words()\n",
        "\n",
        "# Display a few tagged words from each corpus\n",
        "print(\"Brown Corpus Tagged Words:\")\n",
        "print(brown_tagged_words[:10])\n",
        "\n",
        "print(\"\\nCoNLL 2000 Corpus Tagged Words:\")\n",
        "print(conll2000_tagged_words[:10])\n",
        "\n",
        "print(\"\\nTreebank Corpus Tagged Words:\")\n",
        "print(treebank_tagged_words[:10])\n",
        "\n",
        "# Load the tagged sentences from the Treebank corpus\n",
        "treebank_tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "# Display a few tagged sentences from the Treebank corpus\n",
        "print(\"\\nTreebank Corpus Tagged Sentences:\")\n",
        "print(treebank_tagged_sents[:2])\n"
      ],
      "metadata": {
        "id": "waClVsOM86yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)WAP to find the most frequent noun tags."
      ],
      "metadata": {
        "id": "PAkP5JMw8ZEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "id": "4NfNJvriABhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank\n",
        "wsj =treebank.tagged_words()\n",
        "word_tag = nltk.FreqDist(wsj)\n",
        "[word for (word,tag) in word_tag if tag.startswith('N')]"
      ],
      "metadata": {
        "id": "UQnsFKJM8HTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL NO. 4"
      ],
      "metadata": {
        "id": "M6B9w0bDCv-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Map Words to Properties using Python Dictionaries"
      ],
      "metadata": {
        "id": "066YYDOKDQyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADJ'}\n",
        "pos={}\n",
        "pos['colorless'] ='ADJ'\n",
        "pos['ideas'] = 'N'\n",
        "pos['sleep'] ='V'\n",
        "pos['furiously']='ADJ'\n"
      ],
      "metadata": {
        "id": "X2ZunOc47zQr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(pos)"
      ],
      "metadata": {
        "id": "dednApTME3Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pos)"
      ],
      "metadata": {
        "id": "Be8y1gFKFXut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[w for w in pos if w.endswith('s')]"
      ],
      "metadata": {
        "id": "WHh9QiiIFb9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sorted(pos):\n",
        "  print(word, pos[word])"
      ],
      "metadata": {
        "id": "ajvMqbTAFzzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos.keys()"
      ],
      "metadata": {
        "id": "rStbiATCGIy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos.values()"
      ],
      "metadata": {
        "id": "VgAA0hXLGX1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos.items()"
      ],
      "metadata": {
        "id": "XlzKwNaOGbZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos['sleep']=['N','V']\n",
        "pos"
      ],
      "metadata": {
        "id": "XplAZdEueKZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Study (i)DefaultTagger (ii) Regular Expression Tagger (iii) UnigramTagger."
      ],
      "metadata": {
        "id": "gU4S-iNjepIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "# Get tags from the 'news' category in the Brown corpus\n",
        "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
        "\n",
        "# Find the most common tag in the 'news' category\n",
        "most_common_tag = nltk.FreqDist(tags).max()\n",
        "\n",
        "# Print the most common tag\n",
        "print(most_common_tag)  # This will output 'NN'\n",
        "\n",
        "# Raw text for tagging\n",
        "raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "\n",
        "# Create a DefaultTagger with the most common tag\n",
        "default_tagger = nltk.DefaultTagger(most_common_tag)\n",
        "\n",
        "# Tag the tokens with the DefaultTagger\n",
        "tagged_tokens = default_tagger.tag(tokens)\n",
        "\n",
        "# Print the tagged tokens\n",
        "print(tagged_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hqhIWeS3eYJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the tagger using the 'news' category in the Brown corpus\n",
        "gold_standard_tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
        "accuracy = default_tagger.evaluate(gold_standard_tags)\n",
        "\n",
        "# Print the accuracy\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "-YzdkbAIi96k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PRACTICAL NO. 5"
      ],
      "metadata": {
        "id": "8Rbs8wfVBnCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms."
      ],
      "metadata": {
        "id": "xXY3_GUIBuzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(wn.synsets('motocar'))\n",
        "print(wn.synsets('car'))\n",
        "\n",
        "car_synset = wn.synset('car.n.01')\n",
        "print(car_synset.lemma_names())\n",
        "print(car_synset.examples())\n",
        "print(car_synset.definition())\n",
        "print(car_synset.lemmas())\n",
        "print(car_synset.lemmas()[0].name())\n",
        "print(wn.lemma('car.n.01.automobile').synset())\n",
        "print(wn.lemma('car.n.01.automobile').name())\n",
        "print(wn.lemmas('car'))\n",
        "\n",
        "for synset in wn.synsets('car'):\n",
        "    print(synset.lemma_names())\n"
      ],
      "metadata": {
        "id": "7v-G7xhNBXxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " b) Study of lemmas, hyponyms, hypernyms, meronyms, entailments."
      ],
      "metadata": {
        "id": "CyM2SOhzC-Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "car = wn.synset('car.n.01')\n",
        "print(car)\n",
        "\n",
        "types_of_car = car.hyponyms()\n",
        "print(types_of_car)\n",
        "\n",
        "print(types_of_car[26])\n",
        "\n",
        "print(sorted([lemma.name() for synset in types_of_car for lemma in synset.lemmas()]))\n",
        "\n",
        "print(car.hypernyms())\n",
        "paths = car.hypernym_paths()\n",
        "print(len(paths))\n",
        "print([synset.name() for synset in paths[0]])\n",
        "print(car.root_hypernyms())\n",
        "print(wn.synset('tree.n.02').part_meronyms())\n",
        "print(wn.synset('tree.n.01').substance_meronyms())\n",
        "print(wn.synset('tree.n.01').member_holonyms())\n",
        "print(wn.synset('walk.v.01').entailments())\n",
        "print(wn.synset('eat.v.01').entailments())\n",
        "print(wn.lemma('supply.n.02.supply').antonyms())\n",
        "print(wn.lemma('rush.n.02.rush').antonyms())\n",
        "print(wn.lemma('horizontal.a.01.horizontal').antonyms())\n"
      ],
      "metadata": {
        "id": "No9oqWl-B1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) WAP to find synonym and antonym of word 'active' using Wordnet."
      ],
      "metadata": {
        "id": "ou6OjQd7DApF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "print(wordnet.synsets(\"active\"))\n",
        "print(wordnet.synset('active.a.01').lemmas()[0].antonyms())\n"
      ],
      "metadata": {
        "id": "Gyx-DkjpDBIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PRACTICAL NO. 6"
      ],
      "metadata": {
        "id": "WUxztD6MDjkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Compare two nouns.\n",
        "b) Handling stopword"
      ],
      "metadata": {
        "id": "5N64nbI-DmUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "syn1 = wordnet.synsets(\"football\")\n",
        "syn2 = wordnet.synsets('soccer')\n",
        "\n",
        "for s1 in syn1:\n",
        "    for s2 in syn2:\n",
        "        print(\"Path similarity of:\")\n",
        "        print(s1, '(', s1.pos(), ')', '[', s1.definition(), ']')\n",
        "        print(s2, '(', s2.pos(), ')', '[', s2.definition(), ']')\n",
        "        print(\" is\", s1.path_similarity(s2))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "lYRKrNetDI4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)Adding or Removing Stop Words in NLTK’s Default Stop Word List"
      ],
      "metadata": {
        "id": "Xhhlt504EA0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(stopwords.words())\n",
        "\n",
        "text = \"messi likes to play football, however he is not too fond of tennis\"\n",
        "text_tokens = word_tokenize(text)\n",
        "\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in stopwords.words('english')]\n",
        "print(token_without_sw)\n",
        "\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.append('play')\n",
        "\n",
        "text_tokens = word_tokenize(text)\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords]\n",
        "print(token_without_sw)\n",
        "\n",
        "all_stopwords.remove('is')\n",
        "\n",
        "text_tokens = word_tokenize(text)\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords]\n",
        "print(token_without_sw)\n"
      ],
      "metadata": {
        "id": "msyPzjfbDwJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Using Gensim Adding and Removing Stop Words in Default Gensim Stop Words List"
      ],
      "metadata": {
        "id": "8saQ44j0E2AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"messi likes to play football, however he is not too fond of tennis\"\n",
        "filtered_sentence = remove_stopwords(text)\n",
        "print(filtered_sentence)\n",
        "\n",
        "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "print(all_stopwords)\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "all_stopwords_gensim = STOPWORDS.union({'likes', 'play'})\n",
        "text = \"messi likes to play football, however he is not too fond of tennis\"\n",
        "text_tokens = word_tokenize(text)\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords_gensim]\n",
        "print(token_without_sw)\n",
        "\n",
        "all_stopwords_gensim = STOPWORDS\n",
        "sw_list = {\"not\"}\n",
        "all_stopwords_gensim = all_stopwords_gensim.difference(sw_list)\n",
        "print(all_stopwords_gensim)\n",
        "\n",
        "text = \"messi likes to play football, however he is not too fond of tennis\"\n",
        "text_tokens = word_tokenize(text)\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords_gensim]\n",
        "print(token_without_sw)\n"
      ],
      "metadata": {
        "id": "T4Jdkd2HEgYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Using Spacy Adding and Removing Stop Words in Default Spacy Stop Words List"
      ],
      "metadata": {
        "id": "AlQqV3KmFR_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "all_stopwords.add(\"play\")\n",
        "\n",
        "text = \"messi likes to play football, however he is not too fond of tennis\"\n",
        "text_tokens = word_tokenize(text)\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords]\n",
        "print(token_without_sw)\n",
        "\n",
        "all_stopwords.remove('is')\n",
        "token_without_sw = [word for word in text_tokens if word.lower() not in all_stopwords]\n",
        "print(token_without_sw)\n"
      ],
      "metadata": {
        "id": "XmB3qSuxFP5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PRACTICAL NO. 7"
      ],
      "metadata": {
        "id": "28NPVPIsHL4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)Tokenization using Python’s split() function"
      ],
      "metadata": {
        "id": "so9c8mA5HSt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"this tool is on a beta stage, alexa developers can use get metrics\"\n",
        "data=text.split()\n",
        "for i in data:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "Xyts6XIvHCFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Tokenization using Regular Expressions (Regfx)"
      ],
      "metadata": {
        "id": "_FxOXUz0HizF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tk=RegexpTokenizer('s+',gaps=True)\n",
        "str=\"i love to study NLP in Python\"\n",
        "tokens=tk.tokenize(str)\n",
        "print(tokens)\n",
        "['i love to ', 'tudy NLP in Python']"
      ],
      "metadata": {
        "id": "gw29HdOsHYTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Tokenization using NLTK"
      ],
      "metadata": {
        "id": "YQIal69UHnD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "str = \"i love to study DL\"\n",
        "print(word_tokenize(str))\n"
      ],
      "metadata": {
        "id": "6f4J9MipHeS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Tokenization using the spaCy library"
      ],
      "metadata": {
        "id": "359GKd0uID3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.blank(\"en\")\n",
        "str=\"i love nlp\"\n",
        "doc=nlp(str)\n",
        "words=[word.text for word in doc]\n",
        "print(words)\n",
        ""
      ],
      "metadata": {
        "id": "6iES8ZkIH5q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e) Tokenization using Keras\n"
      ],
      "metadata": {
        "id": "-iGffcGWIMPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "str=\"i love to study NLP\"\n",
        "tokens=text_to_word_sequence(str)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "aGDkCsb4IIyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f) Tokenization using Gensim"
      ],
      "metadata": {
        "id": "dsnGiFNvIOFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim"
      ],
      "metadata": {
        "id": "iP-umrSzIZVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "str=\"i love to study nlp\"\n",
        "list(tokenize(str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql3dkn_hIOb3",
        "outputId": "91d8d564-6cf4-47c7-ac93-f635ce136332"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'to', 'study', 'nlp']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PRACTICAL NO. 8"
      ],
      "metadata": {
        "id": "FenP7XVYInKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import NLP Libraries for Indian Languages and perform:"
      ],
      "metadata": {
        "id": "UI1S5MULIspG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Word tokenization in Hindi."
      ],
      "metadata": {
        "id": "wSmhJOrYJtz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "NHlh1iqgIlKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inltk\n",
        "!pip install tornado==4.5.3"
      ],
      "metadata": {
        "id": "Ky8Z-lqbI1u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typing-extensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR4TXFY1Jiiu",
        "outputId": "117540f4-e368-407c-85e2-a4903100dcfd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inltk.inltk import setup\n",
        "setup('hi')\n",
        "from inltk.inltk import tokenize\n",
        "hindi_text = \"\"\"प्राकृ तिक भाषा सीखना बहुत तिलचस्प है।\"\"\"\n",
        "# tokenize(input text, language code)\n",
        "tokenize(hindi_text, \"hi\")"
      ],
      "metadata": {
        "id": "odKavIKLIxlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Generate similar sentences from a given Hindi text input."
      ],
      "metadata": {
        "id": "nAT8hj7SJxLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "!pip install tornado==4.5.3\n",
        "from inltk.inltk import setup\n",
        "setup('hi')\n",
        "from inltk.inltk import get_similar_sentences\n",
        "# get similar sentences to the one given in hindi\n",
        "output = get_similar_sentences('मैंआज बहुत खुश हूं', 5, 'hi')\n",
        "print(output)"
      ],
      "metadata": {
        "id": "kwVVNmdLJZgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Identify the Indian language of a text."
      ],
      "metadata": {
        "id": "50kqey75J5Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "!pip install tornado==4.5.3\n",
        "from inltk.inltk import setup\n",
        "setup('gu')\n",
        "from inltk.inltk import identify_language\n",
        "#Identify the Lnaguage of given text\n",
        "identify_language('બીના કાપડિયા')"
      ],
      "metadata": {
        "id": "jn-c4DorJ31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PRACTICAL NO. 9"
      ],
      "metadata": {
        "id": "4Uraz3bwKS_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIM ->Illustrate POS tagging:"
      ],
      "metadata": {
        "id": "_qh6j1e6KV6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Sentence tokenization, word tokenization, part of speech tagging and chunking of user define text."
      ],
      "metadata": {
        "id": "_qTLY_WiKZP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk import tag\n",
        "from nltk import chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "para = \"Today we will be learning NLTK.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n======\\n\", sents)\n",
        "print(\"\\nword tokenization\\n=========\\n\")\n",
        "for index in range(len(sents)):\n",
        "    words = tokenize.word_tokenize(sents[index])\n",
        "    print(words)\n",
        "\n",
        "# POS Tagging\n",
        "print(\"\\nPOS tagging\\n==========\\n\")\n",
        "tagged_words = []\n",
        "for index in range(len(sents)):\n",
        "    tagged_words.append(tag.pos_tag(tokenize.word_tokenize(sents[index])))\n",
        "print(tagged_words)\n",
        "\n",
        "# chunking\n",
        "print(\"\\nChunking\\n==========\\n\")\n",
        "tree = []\n",
        "for index in range(len(sents)):\n",
        "    tree.append(chunk.ne_chunk(tag.pos_tag(tokenize.word_tokenize(sents[index]))))\n",
        "print(tree)\n"
      ],
      "metadata": {
        "id": "ipacFf5FKUqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Name Entity Recognition of user defined text"
      ],
      "metadata": {
        "id": "La37OEH1Kyxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Apple Inc., originally named Apple Computer, Inc.,\n",
        "is a multinational corporation that creates and markets consumer electronics and attendant computer software, and is a digital distributor of media content. Apple's core product lines are the iPhone smartphone, iPad tablet computer, and the Macintosh personal computer.\n",
        "The company offers its products online and has a chain of retail stores known as Apple Stores. Founders Steve Jobs, Steve Wozniak, and Ronald Wayne created Apple Computer Co. on April 1, 1976, to market Wozniak's Apple I desktop computer, [2] and Jobs and Wozniak incorporated the company on January 3, 1977, [3] in Cupertino, California.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Noun phrases: \", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs: \", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n"
      ],
      "metadata": {
        "id": "fFwU7XQGKoBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Name Entity Recognition with diagram using NLTK corpus-treebank."
      ],
      "metadata": {
        "id": "lK9EXTnQK_o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"treebank\")\n",
        "\n",
        "from nltk.corpus import treebank_chunk\n",
        "\n",
        "\n",
        "treebank_chunk.tagged_sents()[0]\n"
      ],
      "metadata": {
        "id": "bv4DzNUtK51Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_chunk.chunked_paras()[0]"
      ],
      "metadata": {
        "id": "Wuz5lxc9LHsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_chunk.chunked_words()"
      ],
      "metadata": {
        "id": "HLjmCv4GMosh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}